# image_captions

In this project a CNN-RNN encoder-decoder model is trained on the MSCOCO dataset to predict captions for an input image. 
During training the input image is embedded by a pretrained CNN, while the caption is embedded by an embedding layer learned by the model. 
Both embeddings are then fed into The decoder 
